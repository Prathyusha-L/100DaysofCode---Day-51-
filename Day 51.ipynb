{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step1: Sentence Segmentation\n",
    "Step2: Word Tokenization\n",
    "Step3: Stemming\n",
    "Step 4: Lemmatization\n",
    "Step 5: Identifying Stop Words\n",
    "Step 6: Dependency Parsing\n",
    "Step 7: POS tags\n",
    "Step 8: Named Entity Recognition (NER)\n",
    "Step 9: Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'Brazil',\n",
       " 'they',\n",
       " 'drive',\n",
       " 'on',\n",
       " 'the',\n",
       " 'right-hand',\n",
       " 'side',\n",
       " 'of',\n",
       " 'the',\n",
       " 'road',\n",
       " '.',\n",
       " 'Brazil',\n",
       " 'has',\n",
       " 'a',\n",
       " 'large',\n",
       " 'coastli']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"In Brazil they drive on the right-hand side of the road. Brazil has a large coastli\"\n",
    "from nltk.tokenize import word_tokenize# Passing the string text into word tokenize \n",
    "token = word_tokenize(text)\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'Brazil': 2, 'the': 2, 'In': 1, 'they': 1, 'drive': 1, 'on': 1, 'right-hand': 1, 'side': 1, 'of': 1, 'road': 1, ...})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finding the frequency distinct in the tokens\n",
    "# Importing FreqDist library from nltk and passing token into FreqDist\n",
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist(token)\n",
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Brazil', 2),\n",
       " ('the', 2),\n",
       " ('In', 1),\n",
       " ('they', 1),\n",
       " ('drive', 1),\n",
       " ('on', 1),\n",
       " ('right-hand', 1),\n",
       " ('side', 1),\n",
       " ('of', 1),\n",
       " ('road', 1)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To find the frequency of top 10 words\n",
    "fdist1 = fdist.most_common(10)\n",
    "fdist1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wait'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing Porterstemmer from nltk library\n",
    "# Checking for the word ‘waiting’\n",
    "from nltk.stem import PorterStemmer\n",
    "pst = PorterStemmer()\n",
    "pst.stem(\"waiting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "giving:giv\n",
      "given:giv\n",
      "given:giv\n",
      "gave:gav\n"
     ]
    }
   ],
   "source": [
    "# Importing LancasterStemmer from nltk\n",
    "from nltk.stem import LancasterStemmer\n",
    "lst = LancasterStemmer()\n",
    "stm = [\"giving\", \"given\", \"given\", \"gave\"]\n",
    "for word in stm :\n",
    "    print(word+ \":\" +lst.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, lemmatization would correctly identify the base form of ‘caring’ to ‘care’, whereas, stemming\n",
    "would cutoff the ‘ing’ part and convert it to a car.\n",
    "Lemmatization can be implemented in python by using Wordnet Lemmatizer, Spacy Lemmatizer,\n",
    "TextBlob, Stanford CoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rock\n",
      "corpora : corpus\n"
     ]
    }
   ],
   "source": [
    "# Importing Lemmatizer library from nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“Stop words” are the most common words in a language like “the”, “a”, “at”, “for”, “above”, “on”, “is”,\n",
    "“all”. These words do not provide any meaning and are usually removed from texts. We can remove these stop\n",
    "words using nltk library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cristiano', 'ronaldo', 'was', 'born', 'on', 'february', '5', ',', '1985', ',', 'in', 'funchal', ',', 'madeira', ',', 'portugal', '.']\n",
      "['cristiano', 'ronaldo', 'born', 'february', '5', ',', '1985', ',', 'funchal', ',', 'madeira', ',', 'portugal', '.']\n"
     ]
    }
   ],
   "source": [
    "# importing stopwords from nltk library\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "a = set(stopwords.words('english'))\n",
    "text = \"Cristiano Ronaldo was born on February 5, 1985, in Funchal, Madeira, Portugal.\"\n",
    "text1 = word_tokenize(text.lower())\n",
    "print(text1)\n",
    "stopwords = [x for x in text1 if x not in a]\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of speech tagging (POS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part-of-speech tagging is used to assign parts of speech to each word of a given text (such as nouns, verbs,\n",
    "pronouns, adverbs, conjunction, adjectives, interjection) based on its definition and its context. There are many\n",
    "tools available for POS taggers and some of the widely used taggers are NLTK, Spacy, TextBlob, Standford\n",
    "CoreNLP, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('vote', 'NN')]\n",
      "[('to', 'TO')]\n",
      "[('choose', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('particular', 'JJ')]\n",
      "[('man', 'NN')]\n",
      "[('or', 'CC')]\n",
      "[('a', 'DT')]\n",
      "[('group', 'NN')]\n",
      "[('(', '(')]\n",
      "[('party', 'NN')]\n",
      "[(')', ')')]\n",
      "[('to', 'TO')]\n",
      "[('represent', 'NN')]\n",
      "[('them', 'PRP')]\n",
      "[('in', 'IN')]\n",
      "[('parliament', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "text = \"vote to choose a particular man or a group (party) to represent them in parliament\"\n",
    "#Tokenize the text\n",
    "tex = word_tokenize(text)\n",
    "for token in tex:\n",
    "    print(nltk.pos_tag([token]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name entitity recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is the process of detecting the named entities such as the person name, the location\n",
    "name, the company name, the quantities and the monetary value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ghostscript in c:\\users\\prathyu lachireddy\\anaconda3\\lib\\site-packages (0.7)\n",
      "Requirement already satisfied: setuptools>=38.6.0 in c:\\users\\prathyu lachireddy\\anaconda3\\lib\\site-packages (from ghostscript) (50.3.1.post20201107)\n"
     ]
    }
   ],
   "source": [
    "!pip install ghostscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The Ghostscript executable isn't found.\n",
      "See http://web.mit.edu/ghostscript/www/Install.htm\n",
      "If you're using a Mac, you can try installing\n",
      "https://docs.brew.sh/Installation then `brew install ghostscript`\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tree.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    797\u001b[0m                     [\n\u001b[1;32m--> 798\u001b[1;33m                         find_binary(\n\u001b[0m\u001b[0;32m    799\u001b[0m                             \u001b[1;34m\"gs\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_binary\u001b[1;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[0;32m    687\u001b[0m ):\n\u001b[1;32m--> 688\u001b[1;33m     return next(\n\u001b[0m\u001b[0;32m    689\u001b[0m         find_binary_iter(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_binary_iter\u001b[1;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[0;32m    672\u001b[0m     \"\"\"\n\u001b[1;32m--> 673\u001b[1;33m     for file in find_file_iter(\n\u001b[0m\u001b[0;32m    674\u001b[0m         \u001b[0mpath_to_bin\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearchpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_file_iter\u001b[1;34m(filename, env_vars, searchpath, file_names, url, verbose, finding_dir)\u001b[0m\n\u001b[0;32m    631\u001b[0m         \u001b[0mdiv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"=\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m75\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 632\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\\n%s\\n%s\\n%s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n\n===========================================================================\nNLTK was unable to find the gs file!\nUse software specific configuration paramaters or set the PATH environment variable.\n===========================================================================",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tree.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    815\u001b[0m                 )\n\u001b[0;32m    816\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpre_error_message\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 817\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    819\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tree('S', [Tree('PERSON', [('Google', 'NNP')]), ('’', 'NNP'), ('s', 'VBD'), Tree('ORGANIZATION', [('CEO', 'NNP'), ('Sundar', 'NNP'), ('Pichai', 'NNP')]), ('introduced', 'VBD'), ('the', 'DT'), ('new', 'JJ'), ('Pixel', 'NNP'), ('at', 'IN'), Tree('ORGANIZATION', [('Minnesota', 'NNP'), ('Roi', 'NNP'), ('Centre', 'NNP')]), ('Event', 'NNP')])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Google’s CEO Sundar Pichai introduced the new Pixel at Minnesota Roi Centre Event\"\n",
    "#importing chunk library from nltk\n",
    "from nltk import ne_chunk# tokenize and POS Tagging before doing chunk\n",
    "token = word_tokenize(text)\n",
    "tags = nltk.pos_tag(token)\n",
    "chunk = ne_chunk(tags)\n",
    "chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunking means picking up individual pieces of information and grouping them into bigger pieces. In the\n",
    "context of NLP and text mining, chunking means a grouping of words or tokens into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S We/PRP saw/VBD (NP the/DT yellow/JJ dog/NN))\n"
     ]
    }
   ],
   "source": [
    "text = \"We saw the yellow dog\"\n",
    "token = word_tokenize(text)\n",
    "tags = nltk.pos_tag(token)\n",
    "reg = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "a = nltk.RegexpParser(reg)\n",
    "result = a.parse(tags)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SENTIMENTAL ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(polarity=1.0, subjectivity=1.0)\n",
      "Sentiment(polarity=0.9099999999999999, subjectivity=0.7800000000000001)\n",
      "Sentiment(polarity=-0.9099999999999998, subjectivity=0.8666666666666667)\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "Feedback1 = \"The food at the restaurant was awesome\"\n",
    "Feedback2 = \" The food at ABC was very good\"\n",
    "Feedback3 = \"The food was very bad\"\n",
    "blob1 = TextBlob(Feedback1)\n",
    "blob2 = TextBlob(Feedback2)\n",
    "blob3 = TextBlob(Feedback3)\n",
    "print(blob1.sentiment)\n",
    "print(blob2.sentiment)\n",
    "print(blob3.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ayntonyms from WordNet\n",
    "If you remember, we installed NLTK packages using nltk.download().\n",
    "One of the packages was WordNet.\n",
    "WordNet is a database that is built for natural language processing.\n",
    "It includes groups of synonyms and a brief definition.\n",
    "You can get these definitions and examples for a given word like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a symptom of some physical hurt or disorder\n",
      "['the patient developed severe pain and distension']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet \n",
    "syn = wordnet.synsets(\"pain\") \n",
    "print(syn[0].definition()) \n",
    "print(syn[0].examples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the branch of information science that deals with natural language information\n",
      "large Old World boas\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet \n",
    "syn = wordnet.synsets(\"NLP\") \n",
    "print(syn[0].definition()) \n",
    "syn = wordnet.synsets(\"Python\") \n",
    "print(syn[0].definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get antonyms from WordNet .\n",
    "You can get the antonyms words the same way, all you have to do is to check the lemmas before adding them to the array if it’s an antonym or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['small', 'little', 'small']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet \n",
    "antonyms = [] \n",
    "for syn in wordnet.synsets(\"big\"): \n",
    "    \n",
    "    for l in syn.lemmas(): \n",
    "        if l.antonyms(): \n",
    "            antonyms.append(l.antonyms()[0].name()) \n",
    "    \n",
    "print(antonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk#tokenize and POS Tagging before doing chunk\n",
    "\n",
    "text = \"Google's CEO sundar pichai introducted\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VADER sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VADER Sentiment Analysis. VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule\n",
    "based sentiment analysis tool that is specifically attuned to sentiments expressed in social media, and works\n",
    "well on texts from other domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Date: - 28/05/21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Orange is a visual programming software package used for this domain. It has used widely, ranging from machine learning, data mining, and data analysis, etc. Orange tools (called widgets) are within the realm of simple data visualization & pre-processing empirical evaluation of learning algorithms and predictive modelling. Visual programming is implemented via a combination in which workflows are designed by linking user-designed widgets.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Orange is a visual programming software package used for this domain.', 'It has used widely, ranging from machine learning, data mining, and data analysis, etc.', 'Orange tools (called widgets) are within the realm of simple data visualization & pre-processing empirical evaluation of learning algorithms and predictive modelling.', 'Visual programming is implemented via a combination in which workflows are designed by linking user-designed widgets.']\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(text)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Orange', 'is', 'a', 'visual', 'programming', 'software', 'package', 'used', 'for', 'this', 'domain', '.', 'It', 'has', 'used', 'widely', ',', 'ranging', 'from', 'machine', 'learning', ',', 'data', 'mining', ',', 'and', 'data', 'analysis', ',', 'etc', '.', 'Orange', 'tools', '(', 'called', 'widgets', ')', 'are', 'within', 'the', 'realm', 'of', 'simple', 'data', 'visualization', '&', 'pre-processing', 'empirical', 'evaluation', 'of', 'learning', 'algorithms', 'and', 'predictive', 'modelling', '.', 'Visual', 'programming', 'is', 'implemented', 'via', 'a', 'combination', 'in', 'which', 'workflows', 'are', 'designed', 'by', 'linking', 'user-designed', 'widgets', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'.': 4, ',': 4, 'data': 3, 'Orange': 2, 'is': 2, 'a': 2, 'programming': 2, 'used': 2, 'learning': 2, 'and': 2, ...})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = word_tokenize(text)\n",
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist(token)\n",
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['orange', 'is', 'a', 'visual', 'programming', 'software', 'package', 'used', 'for', 'this', 'domain', '.', 'it', 'has', 'used', 'widely', ',', 'ranging', 'from', 'machine', 'learning', ',', 'data', 'mining', ',', 'and', 'data', 'analysis', ',', 'etc', '.', 'orange', 'tools', '(', 'called', 'widgets', ')', 'are', 'within', 'the', 'realm', 'of', 'simple', 'data', 'visualization', '&', 'pre-processing', 'empirical', 'evaluation', 'of', 'learning', 'algorithms', 'and', 'predictive', 'modelling', '.', 'visual', 'programming', 'is', 'implemented', 'via', 'a', 'combination', 'in', 'which', 'workflows', 'are', 'designed', 'by', 'linking', 'user-designed', 'widgets', '.']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-0462295fd555>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtext1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mstopwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-0462295fd555>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtext1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mstopwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "a = set(stopwords.words('english'))\n",
    "text1 = word_tokenize(text.lower())\n",
    "print(text1)\n",
    "stopwords = [x for c in text if x not in a]\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'orange is a visual programming software package used for this domain. it has used widely, ranging from machine learning, data mining, and data analysis, etc. orange tools (called widgets) are within the realm of simple data visualization & pre-processing empirical evaluation of learning algorithms and predictive modelling. visual programming is implemented via a combination in which workflows are designed by linking user-designed widgets.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stemming\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "stemming = PorterStemmer()\n",
    "stemming.stem(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orange is a visual programming software package used for this domain. It has used widely, ranging from machine learning, data mining, and data analysis, etc. Orange tools (called widgets) are within the realm of simple data visualization & pre-processing empirical evaluation of learning algorithms and predictive modelling. Visual programming is implemented via a combination in which workflows are designed by linking user-designed widgets.\n"
     ]
    }
   ],
   "source": [
    "#Lemmantization\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatization = WordNetLemmatizer()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Orange', 'NN')]\n",
      "[('is', 'VBZ')]\n",
      "[('a', 'DT')]\n",
      "[('visual', 'JJ')]\n",
      "[('programming', 'VBG')]\n",
      "[('software', 'NN')]\n",
      "[('package', 'NN')]\n",
      "[('used', 'VBN')]\n",
      "[('for', 'IN')]\n",
      "[('this', 'DT')]\n",
      "[('domain', 'NN')]\n",
      "[('.', '.')]\n",
      "[('It', 'PRP')]\n",
      "[('has', 'VBZ')]\n",
      "[('used', 'VBN')]\n",
      "[('widely', 'RB')]\n",
      "[(',', ',')]\n",
      "[('ranging', 'VBG')]\n",
      "[('from', 'IN')]\n",
      "[('machine', 'NN')]\n",
      "[('learning', 'VBG')]\n",
      "[(',', ',')]\n",
      "[('data', 'NNS')]\n",
      "[('mining', 'NN')]\n",
      "[(',', ',')]\n",
      "[('and', 'CC')]\n",
      "[('data', 'NNS')]\n",
      "[('analysis', 'NN')]\n",
      "[(',', ',')]\n",
      "[('etc', 'NN')]\n",
      "[('.', '.')]\n",
      "[('Orange', 'NN')]\n",
      "[('tools', 'NNS')]\n",
      "[('(', '(')]\n",
      "[('called', 'VBN')]\n",
      "[('widgets', 'NNS')]\n",
      "[(')', ')')]\n",
      "[('are', 'VBP')]\n",
      "[('within', 'IN')]\n",
      "[('the', 'DT')]\n",
      "[('realm', 'NN')]\n",
      "[('of', 'IN')]\n",
      "[('simple', 'NN')]\n",
      "[('data', 'NNS')]\n",
      "[('visualization', 'NN')]\n",
      "[('&', 'CC')]\n",
      "[('pre-processing', 'NN')]\n",
      "[('empirical', 'JJ')]\n",
      "[('evaluation', 'NN')]\n",
      "[('of', 'IN')]\n",
      "[('learning', 'VBG')]\n",
      "[('algorithms', 'NN')]\n",
      "[('and', 'CC')]\n",
      "[('predictive', 'NN')]\n",
      "[('modelling', 'VBG')]\n",
      "[('.', '.')]\n",
      "[('Visual', 'JJ')]\n",
      "[('programming', 'VBG')]\n",
      "[('is', 'VBZ')]\n",
      "[('implemented', 'VBN')]\n",
      "[('via', 'IN')]\n",
      "[('a', 'DT')]\n",
      "[('combination', 'NN')]\n",
      "[('in', 'IN')]\n",
      "[('which', 'WDT')]\n",
      "[('workflows', 'NNS')]\n",
      "[('are', 'VBP')]\n",
      "[('designed', 'VBN')]\n",
      "[('by', 'IN')]\n",
      "[('linking', 'VBG')]\n",
      "[('user-designed', 'JJ')]\n",
      "[('widgets', 'NNS')]\n",
      "[('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "text2 = word_tokenize(text)\n",
    "for token in text2:\n",
    "    print(nltk.pos_tag([token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk\n",
    "text2 = \"sam is driving a car\"\n",
    "token = word_tokenize(text2)\n",
    "tags = nltk.pos_tag(token)\n",
    "chunk=ne_chunk(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The Ghostscript executable isn't found.\n",
      "See http://web.mit.edu/ghostscript/www/Install.htm\n",
      "If you're using a Mac, you can try installing\n",
      "https://docs.brew.sh/Installation then `brew install ghostscript`\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tree.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    797\u001b[0m                     [\n\u001b[1;32m--> 798\u001b[1;33m                         find_binary(\n\u001b[0m\u001b[0;32m    799\u001b[0m                             \u001b[1;34m\"gs\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_binary\u001b[1;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[0;32m    687\u001b[0m ):\n\u001b[1;32m--> 688\u001b[1;33m     return next(\n\u001b[0m\u001b[0;32m    689\u001b[0m         find_binary_iter(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_binary_iter\u001b[1;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[0;32m    672\u001b[0m     \"\"\"\n\u001b[1;32m--> 673\u001b[1;33m     for file in find_file_iter(\n\u001b[0m\u001b[0;32m    674\u001b[0m         \u001b[0mpath_to_bin\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearchpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_file_iter\u001b[1;34m(filename, env_vars, searchpath, file_names, url, verbose, finding_dir)\u001b[0m\n\u001b[0;32m    631\u001b[0m         \u001b[0mdiv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"=\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m75\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 632\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\\n%s\\n%s\\n%s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n\n===========================================================================\nNLTK was unable to find the gs file!\nUse software specific configuration paramaters or set the PATH environment variable.\n===========================================================================",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tree.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    815\u001b[0m                 )\n\u001b[0;32m    816\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpre_error_message\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 817\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    819\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tree('S', [('sam', 'NN'), ('is', 'VBZ'), ('driving', 'VBG'), ('a', 'DT'), ('car', 'NN')])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(polarity=0.0, subjectivity=0.17142857142857143)\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "blob = TextBlob(text)\n",
    "print(blob.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
